\documentclass[12pt,journal,compsoc]{IEEEtran}
\providecommand{\PSforPDF}[1]{#1}
\usepackage{graphicx}
\usepackage{algorithm2e}
\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={black},citecolor={black},pagecolor={black},
urlcolor={black},
pdftitle={DCPT with fallback},
pdfsubject={Prefetcher},
pdfauthor={Leif Tore Rusten, Stian Fredrikstad, Vegar K\aa sli},
pdfkeywords={dcpt,rpt,prefetcher}}
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{DCPT with fallback}
\author{Leif Tore Rusten,
        Stian Fredrikstad,
        and Vegar K\aa sli}

\markboth{DCPT tweaks}%
{DCPT tweaks}

\IEEEcompsoctitleabstractindextext{%
\begin{abstract}
This paper will explain some tweaks to the DCPT algorithm and compare speedups
to the original DCPT
\end{abstract}

\begin{IEEEkeywords}
DCPT, sequential, prefetcher
\end{IEEEkeywords}}

\maketitle
\IEEEdisplaynotcompsoctitleabstractindextext
\IEEEpeerreviewmaketitle

\section{Introduction}
Prefetcher has been a hot topic the last years since the memory gap has become so big.

\section{Methodology}
The test setup consist of the M5 simulator running on NTNU's computer
cluster kongull. M5 is setup to simulate the Alpha 21264 microarchitecture
plus the prefetcher described in this paper. A subset of the SPEC CPU2000
benchmarks where used for measuring speedup. These SPEC benchmarks are assumed
to be representative for real world workloads, making the generated speedup
results relevant for real world applications. This test setup is given
as part of the project work in the course TDT4260 at NTNU. In addition,
shell scripts for brute force search for optimal prefetcher parameters
where created.

The M5 simulator supplied had been modified by the course responsible such
that the programming interface had been abstracted into a simplified framework
allowing for an minimal learning curve and fast prefetcher development. The
framework also contained python scripts to run a lighter version of the
benchmarks on kongull. For tuning the different parameters shell scripts
where created which iterates over the parameter space and runs the heavy
version of the benchmarks, i.e. the same as on kongull. The command line
for M5 execution looks as follows:

\begin{verbatim}                   *
M5_CPU2000=lib/cpu2000
./m5/build/ALPHA_SE/m5.opt
--remote-gdb-port=0 -re --outdir
output/${b} m5/configs/example/se.py
--detailed --checkpoint-dir=lib/cp
--checkpoint-restore=1000000000
--at-instruction --caches --l2cache
--standard-switch --membus-width=8
--warmup-insts=10000000
--max-inst=1000000000 --l2size=1MB
--membus-clock=400MHz
--mem-latency=30ns --bench=${b}
--prefetcher=on_access=true:policy=proxy
\end{verbatim}

The methodology used in the beginning was to start with a sequential
prefetcher and briefly progress through prior art, i.e. an RPT implementation
and finally a DCPT implementation. During development the performance was
measured by running a M5-simulator testbench framework and the obtained test
results where evaluated against previous versions' results for
empirically determining the usefulness of the modification.

The prefetcher development has been driven by ideas from the group and it
has been a low barrier for implementing ideas. The ideas proposed have been
thought to be improvements mostly because of personal or group reflection over
the current prefetching mechanism and the memory hierarchy. Testing of
many different modifications and following performance evaluations have been
done rapidly.

So far the development can be seen as searching somewhat blindly for a good
solution. The proposed modifications have often been based on an insight
about the currently implemented prefetcher's (seq, RPT or DCPT) functionality.
Other modifications are mere tweaks of parameters for the different prefetcher
strategies for fine-tuning the performance metrics. Since the main focus of
our research is on the DCPT prefetcher and the development methodology
outlined above (basicly the group using its creativity) will probably only
give a noteworthy performance increase by chance. And even though such
modifications could have been based on a justified/rational idea spurred by
creativity, it would have been better if they where based on measurements and
novel memory access patterns derived from an analytical method.

This means that a shift of methodology probably is required to make truly
interesting discoveries academically and make it more likely to find and
exploit prefetching opportunities. For the remaining work the use of valgrind
and the inspection of which code snippets give cache misses and trying to
analyse these to find potential underlying factors/mechanisms for these
will become a more important tool as we run out of good a priori
insights/ideas.

\section{Delta Correlating Prediction Table (DCPT) with sequential fallback}
The algorithm for DCPT (Delta Correlating Prediction Table) is based on the pseudo code from \cite{dcptpaper}.
The algorithm is related to RPT, but the difference is that it looks for a pattern instead of one stride.
When the it gets a request, it saves the (PC) program counter, and the stride in a table connected to each PC. When it has more than two elements in the list with strides, it starts looking for patterns.
If it finds a pattern in the list, it saves all the adresses that possibly should be prefetched in another list with candidates for prefetch.
Then it runs through the list and checks if the adresses is not in the cache, in the mshr or that the adress is out of memory. If all the requirements are met, it issues a prefetch for it.

If it has less than two deltas in the list or if it cant find a pattern, it starts prefetching the next block.

Attempts to tweak this implementation:
\begin{itemize}
\item Make a copy of the queue to keep track of which adresses that still aren't fetched, to avoid "double fetching".
\item Make a lru which prevents the prefetcher from throwing out recently used history from the list.
\item Adjustments to the size of the list, and size of the delta list
\end{itemize}

\subsection{Pseudocode}

\begin{algorithm}[H]
\dontprintsemicolon
\KwIn{stat}

\Begin{
 $i \leftarrow TABLE[stat.pc]$\;
 \If{$i \neq 0$}{
  $DELTA \leftarrow stat.mem\_addr - i.last\_mem\_addr$\;
  $i.deltas[ ] \leftarrow DELTA$\;
 }
 $i.last\_mem\_addr \leftarrow stat.mem\_addr$\;
 $TABLE[stat.pc] \leftarrow i$\;
 $candidates \leftarrow []$\;
 \If{$size(I.deltas) > 1$}{
   $candidates \leftarrow DCPT(i)$\;
   $prefetch(candidates)$\;
 }
 \If{$size(candidates) == 0$ {\bf and} $canPrefetch(NEXT\_BLOCK)$}{
  $issue\_prefetch(NEXT\_BLOCK)$\;
 }
}
\caption{prefetch\_access\label{pa}}
\end{algorithm}

\begin{algorithm}[H]
\dontprintsemicolon
\KwIn{addr}

\Begin{
 ${\bf return}$ $addr < MAX\_PHYS\_ADDR$ \\
	{\bf and} $!in\_mshr\_queue(addr)$ \\
	{\bf and} $!in\_cache(addr)$ \\
	{\bf and} $!in\_cache\_queue(addr)$\;
}
\caption{canPrefetch\label{cp}}
\end{algorithm}

\begin{algorithm}[H]
\dontprintsemicolon
\KwIn{i}

\Begin{
 $last\_delta\_i \leftarrow i.deltas.last$\;
 \ForEach{$U, V \in reversed(i.deltas)$}{
  \If{$U == i.deltas[last\_delta\_i]$ {\bf and} $V == i.deltas[last\_delta\_i - 1]$}{
   $match\_i \leftarrow indexof(U)$\;	
   ${\bf break}$\;
  }
 }
 $candidates \leftarrow []$\;
 \While{$match\_i < size(i.deltas)$}{
  $candidates \leftarrow i.last\_mem\_addr$ + $i.deltas[++match\_i]$\;
 }
 ${\bf return}$ $candidates$\;
}
\caption{DCPT\label{dcpt}}
\end{algorithm}

\begin{algorithm}[H]
\dontprintsemicolon
\KwIn{candidates}

\Begin{
 \ForEach{$I \in candidates$}{
  \If{$canPrefetch(I)$}{
   $issuePrefetch(I)$\; 
  }
 }
}
\caption{prefetch\label{pre}}
\end{algorithm}


\section{Results}
\subsection{Delta Correlation Prediction Table}
The first implementation of DCPT was not right according to the pseudocode in \cite{dcptpaper}, but it accomplish a 8.6\% speedup with a table of 100 entries and 10 delta per entry.
The attempts to tweak this with other values have not been successful, but it gives around 8\% speedup.
Attempts to add a check to issue less prefetches does also lead to less speedup.

The next implementation, which should be right according to the pseudocode in \cite{dcptpaper} did not do as well, a maximum of 7.3\% speedup with the same specification as the first implementation was accomplished.

\subsection{Sequential failover}
Look at the pretty plot! This took all to long to generate so the accompanying
informative text will have to wait until I have caught some zzz.

\includegraphics[width=0.45\textwidth]{sequential_failover_speedup_plt}

\section{Discussion}
\subsection{Using LRU to preserve history}
Since there are a 8kB memory limit it was looked into modifying the table in DCPT to use LRU. This was implemented by always storing the Tick that was given with the argument to prefetch\_access and use this to determine which pc that should be replaced by a new. Unfortunately this reduced the total speedup, this is possibly because there are more often hits in the table and that could give more hits in the deltas table, and this leads in some cases to more misses and less sequential prefetching, specifically the twolf which uses some have use of sequential prefetch had reduced speedup.

\subsection{Too aggressive prefetching}
The prefetcher implemented here is very aggressive and gives a lot of identified block, but only issues some percent of these. The reason why a block is identified but not issued is in some cases that the block is already in the queue, because of that a copy of the cache queue was implemented and checked before every prefetch, this helped a little, but there are still too many identified compared to issued. Some of the problem can also be that the prefetcher matches patterns with a large number of deltas between, this leads to that the prefetches identifies too many blocks every time prefetch\_access is called and since the queue is limited, some of the blocks that are actually needed are pushed out of the queue. %\TODO add discussion with results from pfjudge whats up? with real limit
\section{Conclusion}

\section{Further Work}
This section is optional, but might become large.

\subsection{Stuff we will look into before delivery (i.e. our plan forward)}
Continuing to tweak and implement the modified regular DCPT. The plan is
to work with this in March and cut off the gathering of results 1. April
and start focusing on writing the report and presentation.

Look into the generalization of DCPT as a tree-structure. If this gives
ok speedup then we will change the focus of the report into comparing this
approach with our version of DCPT. If performance is low, only mention the
design and do not analyse it further, instead focus on analysing our
DCPT version.

The tree-generalization of DCPT is outlined somewhat as follows. Each new PC
causing an access gets a tree-root-node and two tree-pointers allocated.
As this access is the first access for the PC, no history is available.
Therefore add a child node to the PC's root and associate the stride 1 with it.
Move one of the tree-pointers, cur\_prefetch to this child, and leave the other
pointer, true\_access, alone. At next access for this PC verify if this access
has been correctly prefetched by traversing from cur\_prefetch and up to the
root, checking if any node correspond to the access. If this is the case,
move true\_access to that node. If no such node is found, search the children
of true\_access and check for a stride giving this access. If child found, move
true\_access to this and start prefetching from this nodes subtree and move
cur\_prefetch accordingly. If no child found create a new child node and set
its stride to 1 as in the starting case. These starting or learning cases
are such that if the stride 1 is guessed wrong, correct the stride in the node
instead of creating a new child node, but after this set the node as no longer
learning.

\subsection{Stuff we won't have time to, but that might be good ideas}

%\appendices
%\section{}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgement
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
%  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
%  \section*{Acknowledgment}
\fi


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



\begin{thebibliography}{1}

\bibitem{dcptpaper}
M.~Grann\ae s, M.~Jahre and L.~Natvig, \emph{Storage Efficient Hardware Prefetching using Delta Correlating Prediction Tables}, 2009.

\end{thebibliography}



% that's all folks
\end{document}


